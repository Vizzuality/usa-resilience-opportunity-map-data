{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import LMIPy\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "#import regionmask\n",
    "import zarr\n",
    "import gcsfs\n",
    "import requests\n",
    "import shapely.wkb \n",
    "from shapely.ops import cascaded_union\n",
    "import regionmask\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_carto(account, query):\n",
    "    \"\"\"\n",
    "    It gets data by querying a carto table and converts it into a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    urlCarto = f\"https://{account}.carto.com/api/v2/sql\"\n",
    "    \n",
    "    sql = {\"q\": query}\n",
    "    r = requests.get(urlCarto, params=sql)\n",
    "    \n",
    "    data = r.json()\n",
    "    \n",
    "    df = gpd.GeoDataFrame(data.get(\"rows\"))\n",
    "    if 'the_geom' in df.columns:\n",
    "        # Change geometry from WKB to WKT format\n",
    "        df['geometry'] = df.apply(lambda x: shapely.wkb.loads(x['the_geom'],hex=True), axis=1 )\n",
    "        df.drop(columns=['the_geom'], inplace=True)\n",
    "        if 'the_geom_webmercator' in df.columns:\n",
    "            df.drop(columns=['the_geom_webmercator'], inplace=True)\n",
    "        df.crs = {'init': 'epsg:4326'}\n",
    "        df = df.to_crs({'init': 'epsg:4326'})\n",
    "        \n",
    "    return df\n",
    "\n",
    "def weighted_mean(df, columns=['cfr_raw'], weight_column='area', groupby_on='gid_2'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define a lambda function to compute the weighted mean:\n",
    "    wm = lambda x: np.average(x, weights=df.loc[x.index, weight_column])\n",
    "\n",
    "    # Groupby and aggregate with lambda function:\n",
    "    for n, column in enumerate(columns):\n",
    "        if n == 0:\n",
    "            df_new = df.groupby(groupby_on).agg({column: wm})  \n",
    "        else:\n",
    "            df_new[column] = df.groupby(groupby_on).agg({column: wm})[column]\n",
    "        \n",
    "    return gpd.GeoDataFrame(df_new)\n",
    "\n",
    "\n",
    "def quantile_interp_function(s,q,y):\n",
    "    \"\"\" Get a interpolated function based on quantiles.\n",
    "    y and q should be the same length.\n",
    "    \n",
    "    Args:\n",
    "        s(pandas Series): Input y data that needs to \n",
    "            be remapped.\n",
    "        q(list): list with quantile x values.\n",
    "        y(list): list with y value to map to.\n",
    "        \n",
    "    Returns:\n",
    "        f(interp1d) : Scipy function object.\n",
    "        quantiles(Pandas Series): list of quantile y \n",
    "            values. \n",
    "        \n",
    "    Example:\n",
    "    \n",
    "        s = df[\"col\"]\n",
    "        q = [0,0.2,0.4,0.6,0.8,1]\n",
    "        y = [0,1,2,3,4,5]\n",
    "        f = quantile_interp_function(s,quantiles,y)\n",
    "        y_new = f(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    quantiles = s.quantile(q=q)\n",
    "    print(\"quantiles used for aggregate total:\",quantiles)\n",
    "    f = interpolate.interp1d(quantiles,y)\n",
    "    return f, quantiles\n",
    "\n",
    "def score_to_category(score):\n",
    "    if np.isnan(score):\n",
    "        cat = -1\n",
    "    elif score != 5:\n",
    "        cat = int(np.floor(score))\n",
    "    else:\n",
    "        cat = 4\n",
    "    return cat\n",
    "\n",
    "def decile_to_category(decile):\n",
    "    if np.isnan(decile):\n",
    "        cat = -1\n",
    "    elif (decile >= 0) and (decile <= 2):\n",
    "        cat = 0\n",
    "    elif (decile > 2) and (decile <= 4):\n",
    "        cat = 1\n",
    "    elif (decile > 4) and (decile <= 6):\n",
    "        cat = 2\n",
    "    elif (decile > 6) and (decile <= 8):\n",
    "        cat = 3\n",
    "    elif (decile > 8) and (decile <= 10):\n",
    "        cat = 4\n",
    "    else:\n",
    "        cat = 4\n",
    "    return cat\n",
    "\n",
    "def compute_score_category_label(df, columns, q, scores):\n",
    "    df = df.copy()\n",
    "    for column in columns:\n",
    "        df.sort_values(column, inplace=True)\n",
    "        \n",
    "        # Geometries without a hazard are given the lowest risk score, 0\n",
    "        df_0 = df[df[column] == 0]\n",
    "        df_0[f\"{column}_score\"] = 0.\n",
    "        \n",
    "        df = df[df[column] != 0]\n",
    "        s = df[column]\n",
    "        f, quantiles =quantile_interp_function(s,q,scores)\n",
    "\n",
    "        # Add scores\n",
    "        df[f\"{column}_score\"] = df[column].apply(f)\n",
    "        \n",
    "        df = pd.concat([df_0, df])\n",
    "                                                          \n",
    "        # Add categories\n",
    "        df[f\"{column}_cat\"] = df[f\"{column}_score\"].apply(score_to_category)\n",
    "        \n",
    "        # Add range\n",
    "        quantiles = list(quantiles)\n",
    "        df_tmp1 = df[df[f\"{column}_cat\"] == -1]\n",
    "        df_tmp1[f\"{column}_range\"] = 'Data not available'\n",
    "        df_tmp2 = df[df[f\"{column}_cat\"] != -1]\n",
    "        df_tmp2[f\"{column}_range\"] = df_tmp2[f\"{column}_cat\"].apply(lambda x: str([quantiles[x],quantiles[x+1]]))\n",
    "        \n",
    "        df = pd.concat([df_tmp1, df_tmp2])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def compute_category_label(df, columns):\n",
    "    df = df.copy()\n",
    "    for column in columns:\n",
    "        df.sort_values(column, inplace=True)\n",
    "                                                          \n",
    "        # Add categories\n",
    "        df[f\"{column}_cat\"] = df[f\"{column}\"].apply(score_to_category)\n",
    "        \n",
    "        # Add range\n",
    "        ranges = [0,1,2,3,4,5]\n",
    "        df_tmp1 = df[df[f\"{column}_cat\"] == -1]\n",
    "        df_tmp1[f\"{column}_range\"] = 'Data not available'\n",
    "        df_tmp2 = df[df[f\"{column}_cat\"] != -1]\n",
    "        df_tmp2[f\"{column}_range\"] = df_tmp2[f\"{column}_cat\"].apply(lambda x: str([ranges[x],ranges[x+1]]))\n",
    "        \n",
    "        df = pd.concat([df_tmp1, df_tmp2])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def compute_decile_category_label(df, columns):\n",
    "    df = df.copy()\n",
    "    for column in columns:\n",
    "        # Add scores\n",
    "        df[f\"{column}_decile\"] = df[column].apply(lambda x: int(np.round(x)))\n",
    "                                                          \n",
    "        # Add categories\n",
    "        df[f\"{column}_cat\"] = df[f\"{column}_decile\"].apply(decile_to_category)\n",
    "        \n",
    "        # Add range\n",
    "        ranges = [0,2,4,6,8,10]\n",
    "        df_tmp1 = df[df[f\"{column}_cat\"] == -1]\n",
    "        df_tmp1[f\"{column}_range\"] = 'Data not available'\n",
    "        df_tmp2 = df[df[f\"{column}_cat\"] != -1]\n",
    "        df_tmp2[f\"{column}_range\"] = df_tmp2[f\"{column}_cat\"].apply(lambda x: str([ranges[x],ranges[x+1]]))\n",
    "        \n",
    "        df = pd.concat([df_tmp1, df_tmp2])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def compute_percentile(df, columns):\n",
    "    df = df.copy()\n",
    "    q = [0,0.2,0.4,0.6,0.8,1]\n",
    "    scores = [0,0.2,0.4,0.6,0.8,1]\n",
    "    for column in columns:\n",
    "        df.sort_values(column, inplace=True)\n",
    "        \n",
    "        s = df[column]\n",
    "        f, quantiles =quantile_interp_function(s,q,scores)\n",
    "\n",
    "        # Add percentile\n",
    "        df[column.replace('EP',  'EPL')] = df[column].apply(f)                                                      \n",
    "        \n",
    "    return df\n",
    "\n",
    "def set_lat_lon_attrs(ds):\n",
    "    \"\"\" Set CF latitude and longitude attributes\"\"\"\n",
    "    ds[\"lon\"] = ds.lon.assign_attrs({\n",
    "      'axis' : 'X',\n",
    "       'long_name' : 'longitude',\n",
    "        'standard_name' : 'longitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_east',\n",
    "            'valid_max' : 360.0,\n",
    "             'valid_min' : -180.0\n",
    "             })\n",
    "    ds[\"lat\"] = ds.lat.assign_attrs({\n",
    "      'axis' : 'Y',\n",
    "       'long_name' : 'latitude',\n",
    "        'standard_name' : 'latitude',\n",
    "         'stored_direction' : 'increasing',\n",
    "          'type' : 'double',\n",
    "           'units' : 'degrees_north',\n",
    "            'valid_max' : 90.0,\n",
    "             'valid_min' : -90.0\n",
    "             })\n",
    "    return ds\n",
    "\n",
    "def create_ds_mask(df, ds, name, lon_name='lon', lat_name='lat'):\n",
    "    # Create index column\n",
    "    if 'index' not in df:\n",
    "        df = df.reset_index(drop=True).reset_index()\n",
    "    \n",
    "    # Get mean ds cell area (in degrees) \n",
    "    mean_y_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_y_size)\n",
    "    mean_x_size = np.diff(ds.lat.values).mean()\n",
    "    #print(mean_x_size)\n",
    "    mean_area = mean_y_size * mean_x_size\n",
    "    print(f\"The mean ds cell area is {np.round(mean_area, 6)} deg.\\n\")\n",
    "    \n",
    "    # Clip gdf to bounding box of ds\n",
    "    xmin = ds.lon.min().values.tolist()\n",
    "    xmax = ds.lon.max().values.tolist()\n",
    "    ymin = ds.lat.min().values.tolist()\n",
    "    ymax = ds.lat.max().values.tolist()\n",
    "    df = df.cx[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    \n",
    "    # Add area of geoms to gdf\n",
    "    df = df.assign(area = df.area)\n",
    "    df = df.assign(area_is_gt_cell = df['area'] > mean_area)\n",
    "    print(f\"Clipped gdf to dataset bounds, giving {len(df['index'])} potential geometries, of which {df['area_is_gt_cell'].sum()} are large enough.\\n\")\n",
    "    \n",
    "    print(\"Geometries smaller than mean cell size:\")\n",
    "    print(df.loc[df['area_is_gt_cell'] == False, ['index']])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Extract indexes and geoms that are large enough!\n",
    "    id_ints = df.loc[df['area_is_gt_cell'] == True, 'index'].values\n",
    "    geoms = df.loc[df['area_is_gt_cell'] == True, 'geometry'].values\n",
    "    \n",
    "    print(f'Number of indexes: {len(id_ints)}')\n",
    "    print(f'Number of geoms: {len(geoms)}')\n",
    "\n",
    "    # create mask object\n",
    "    da_mask = regionmask.Regions(\n",
    "      name = name,\n",
    "      numbers = id_ints,\n",
    "      outlines = geoms)\\\n",
    "      .mask(ds, lon_name=lon_name, lat_name=lat_name)\\\n",
    "      .rename(name)\n",
    "\n",
    "    # get the ints actually written to mask\n",
    "    id_ints_mask = da_mask.to_dataframe().dropna()[name].unique()\n",
    "    id_ints_mask = np.sort(id_ints_mask).astype('int')\n",
    "    \n",
    "    print(f'Number of ints in mask: {len(id_ints_mask)}')\n",
    "    \n",
    "    # update da attributes\n",
    "    da_mask.attrs['id_ints'] = id_ints_mask\n",
    "    da_mask = set_lat_lon_attrs(da_mask)\n",
    "    return da_mask, df[df['area_is_gt_cell'] == False]['index']\n",
    "\n",
    "def rtree_intersect(gdf, geometries):\n",
    "    number_fires = []\n",
    "    mean_fire_size = []\n",
    "    total_fire_size = []\n",
    "    per_total_fire_size = []\n",
    "    sindex = gdf.sindex\n",
    "    # Iterate over the geometries\n",
    "    for n, geometry in enumerate(tqdm(geometries.geometry)):\n",
    "        # Find approximate matches with r-tree\n",
    "        possible_matches_index = list(sindex.intersection(geometry.bounds))\n",
    "        possible_matches = gdf.iloc[possible_matches_index]\n",
    "        # Find precise matches with r-tree\n",
    "        precise_matches = possible_matches[possible_matches.intersects(geometry)]\n",
    "        precise_matches['geometry'] = precise_matches.intersection(geometry)\n",
    "        \n",
    "        if not precise_matches.empty:\n",
    "            # Add area in ha\n",
    "            precise_matches['area_ha'] = precise_matches['geometry'].to_crs({'init': 'epsg:32633'}).map(lambda p: p.area / 10**4)\n",
    "            number_fires.append(len(precise_matches))\n",
    "            mean_fire_size.append(precise_matches['area_ha'].mean())\n",
    "            \n",
    "            total_fire_size.append(cascaded_union(precise_matches['geometry'].to_crs({'init': 'epsg:32633'})).area / 10**4)\n",
    "            per_total_fire_size.append(cascaded_union(precise_matches['geometry']).area/geometry.area*100)\n",
    "        else:\n",
    "            number_fires.append(0)\n",
    "            mean_fire_size.append(0.)\n",
    "            total_fire_size.append(0.)\n",
    "            per_total_fire_size.append(0.)\n",
    "    return number_fires, mean_fire_size, total_fire_size, per_total_fire_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
